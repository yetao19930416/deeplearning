第四周部分第二周

经典网络没看懂后面补

1、残差网络
 随着神经网络层数越来越多需要残差网络进行性能优化
Resnets由残差块组成

$a^{[l]}$-》Linear-》Relu（$a^{[l+1]}$）-》Linear-》Relu-》$a^{[l+2]}$
$z^{[l+1]}$ = $w^{[l+1]}$$a^{[l]}$+$b^{[l+1]}$
$a^{[l+1]}$ =g（$z^{[l+1]}$）
$z^{[l+2]}$ = $w^{[l+2]}$$a^{[l+1]}$+$b^{[l+2]}$
$a^{[l+2]}$ =g（$z^{[l+2]}$）
残差网络 $a^{[l+2]}$ =g（$z^{[l+2]}$+$a^{[l]}$）

$a^{[l]}$插入时机是在线性激活之后，Relu激活之前
又叫远跳连接
使用残差快可以训练更深的神经网络

x-》Big NN-》$a^{[l]}$

x-》Big NN-》$a^{[l]}$-》fc2-》fc3-》$a^{[l+2]}$
  Relu a》=0

$a^{[l+2]}$ = g（$z^{[l+2]}$+$a^{[l]}$）
$a^{[l+2]}$ = g（$w^{[l+2]}$$a^{[l+1]}$+$b^{[l+2]}$+$a^{[l]}$）
   If $w^{[l+2]}$ = $b^{[l+2]}$ = 0
    $a^{[l+2]}$ = g（$a^{[l]}$）
    证明至少效率不会降低
    如果$a^{[l+2]}$和$a^{[l]}$维度不一样就用WS转
    普通网络和ResNet网络通常的结构是卷积层，卷积层，卷积层，池化层依此重复

1乘以1卷积压缩信道数量并减少计算的
Inception模块用来决定过滤器大小，要不要加池化层都自动帮你处理了
